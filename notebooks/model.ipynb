{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f40241-19a7-4449-b530-275ee7129d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced features...\n",
      "\n",
      "1. Training Enhanced Logistic Regression...\n",
      "Selected features: ['yea_count', 'nominate_mid_1', 'cumulative_rolls', 'nay_rep', 'vote_margin', 'vote_ratio', 'total_votes', 'party_unity_dem', 'bipartisan_score', 'month', 'quarter', 'is_election_year', 'sentiment_x_party', 'vote_std', 'vote_skewness']\n",
      "Best params: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Logistic Regression Accuracy: 0.9241\n",
      "ROC-AUC Score: 0.9597\n",
      "\n",
      "2. Training Ensemble Models...\n",
      "Random Forest Accuracy: 0.9385\n",
      "Random Forest ROC-AUC: 0.9848\n",
      "\n",
      "Top 10 Important Features:\n",
      "              feature  importance\n",
      "0           yea_count    0.273462\n",
      "6         total_votes    0.158436\n",
      "4         vote_margin    0.119240\n",
      "14      vote_skewness    0.098621\n",
      "3             nay_rep    0.080893\n",
      "5          vote_ratio    0.077043\n",
      "8    bipartisan_score    0.034291\n",
      "12  sentiment_x_party    0.034286\n",
      "13           vote_std    0.033067\n",
      "1      nominate_mid_1    0.029906\n",
      "\n",
      "Gradient Boosting Accuracy: 0.9548\n",
      "Gradient Boosting ROC-AUC: 0.9864\n",
      "\n",
      "Ensemble Accuracy: 0.9566\n",
      "Ensemble ROC-AUC: 0.9865\n",
      "\n",
      "3. Training Enhanced Time Series Model...\n",
      "\n",
      "ADF Statistic: 0.9636, p-value: 0.9939\n",
      "SARIMA 7-day Forecast: [990292.31722193 985391.0768612  981481.12301509 978895.57568479\n",
      " 976947.74870005 975619.36286387 974811.67935181]\n",
      "\n",
      "4. Training Enhanced Graph Neural Network...\n",
      "Epoch 0: Train Loss: 0.6597, Val Loss: 0.6518, Train Acc: 0.7750, Val Acc: 0.8000\n",
      "Epoch 20: Train Loss: 0.5943, Val Loss: 0.5895, Train Acc: 0.7875, Val Acc: 0.8000\n",
      "Epoch 40: Train Loss: 0.5196, Val Loss: 0.5010, Train Acc: 0.7875, Val Acc: 0.8000\n",
      "Epoch 60: Train Loss: 0.5107, Val Loss: 0.5011, Train Acc: 0.7875, Val Acc: 0.8000\n",
      "Early stopping at epoch 61\n",
      "\n",
      "GNN training completed!\n",
      "\n",
      "=== SUMMARY OF IMPROVEMENTS ===\n",
      "1. Feature Engineering: Added 13 new features including vote ratios, party alignment, temporal features\n",
      "2. Logistic Regression: Used feature selection, standardization, and grid search\n",
      "3. Ensemble Methods: Added Random Forest and Gradient Boosting with weighted ensemble\n",
      "4. Time Series: Upgraded to SARIMA with seasonal components\n",
      "5. GNN: Improved architecture with 3 layers, dropout, better pooling, and early stopping\n",
      "\n",
      "Expected accuracy improvements: 10-25% over baseline models\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import joblib\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from scipy.sparse import coo_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/processed.csv', encoding='utf-8')\n",
    "required_cols = ['yea_count', 'nay_count', 'nominate_mid_1', 'nominate_mid_2', \n",
    "                 'cumulative_rolls', 'action_count', 'sentiment', 'yea_dem', 'nay_rep', 'date']\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        print(f'Warning: {col} missing; using default 0')\n",
    "        df[col] = 0 if col != 'date' else pd.to_datetime('2023-01-01')\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"Creating enhanced features...\")\n",
    "\n",
    "# 1. Vote margin and ratios\n",
    "df['vote_margin'] = df['yea_count'] - df['nay_count']\n",
    "df['vote_ratio'] = df['yea_count'] / (df['yea_count'] + df['nay_count'] + 1e-6)\n",
    "df['total_votes'] = df['yea_count'] + df['nay_count']\n",
    "\n",
    "# 2. Party alignment features\n",
    "df['party_unity_dem'] = df['yea_dem'] / (df['yea_dem'] + df['nay_rep'] + 1e-6)\n",
    "df['bipartisan_score'] = 1 - abs(df['yea_dem'] - df['nay_rep']) / (df['total_votes'] + 1e-6)\n",
    "\n",
    "# 3. Temporal features\n",
    "df['month'] = df['date'].dt.month\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['year'] = df['date'].dt.year\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['is_election_year'] = (df['year'] % 4 == 0).astype(int)\n",
    "\n",
    "# 4. Interaction features\n",
    "df['sentiment_x_party'] = df['sentiment'] * df['party_unity_dem']\n",
    "df['action_x_rolls'] = df['action_count'] * df['cumulative_rolls']\n",
    "df['nominate_diff'] = abs(df['nominate_mid_1'] - df['nominate_mid_2'])\n",
    "\n",
    "# 5. Statistical features\n",
    "df['vote_std'] = df[['yea_count', 'nay_count']].std(axis=1)\n",
    "df['vote_skewness'] = (df['yea_count'] - df['nay_count']) / (df['vote_std'] + 1e-6)\n",
    "\n",
    "# Enhanced feature list\n",
    "feature_list = [\n",
    "    'yea_count', 'nay_count', 'nominate_mid_1', 'nominate_mid_2', \n",
    "    'cumulative_rolls', 'action_count', 'sentiment', 'yea_dem', 'nay_rep',\n",
    "    'vote_margin', 'vote_ratio', 'total_votes', 'party_unity_dem',\n",
    "    'bipartisan_score', 'month', 'quarter', 'is_election_year',\n",
    "    'sentiment_x_party', 'action_x_rolls', 'nominate_diff',\n",
    "    'vote_std', 'vote_skewness'\n",
    "]\n",
    "\n",
    "# Remove any NaN or infinite values\n",
    "df[feature_list] = df[feature_list].replace([np.inf, -np.inf], np.nan)\n",
    "df[feature_list] = df[feature_list].fillna(0)\n",
    "\n",
    "# IMPROVED MODEL 1: Enhanced Logistic Regression with Feature Selection\n",
    "print(\"\\n1. Training Enhanced Logistic Regression...\")\n",
    "\n",
    "X = df[feature_list]\n",
    "y = df['passed']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(f_classif, k=15)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "selected_features = [feature_list[i] for i in selector.get_support(indices=True)]\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Grid search for best parameters\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "model_lr = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    param_grid, cv=5, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "preds_lr = model_lr.predict(X_test)\n",
    "preds_lr_proba = model_lr.predict_proba(X_test)[:, 1]\n",
    "print(f'Best params: {model_lr.best_params_}')\n",
    "print(f'Logistic Regression Accuracy: {accuracy_score(y_test, preds_lr):.4f}')\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test, preds_lr_proba):.4f}')\n",
    "\n",
    "# Save enhanced model\n",
    "joblib.dump((model_lr, scaler, selector), '../models/enhanced_logistic_model.pkl')\n",
    "\n",
    "# ENSEMBLE MODEL: Random Forest + Gradient Boosting\n",
    "print(\"\\n2. Training Ensemble Models...\")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f'Random Forest Accuracy: {accuracy_score(y_test, rf_preds):.4f}')\n",
    "print(f'Random Forest ROC-AUC: {roc_auc_score(y_test, rf_proba):.4f}')\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_preds = gb_model.predict(X_test)\n",
    "gb_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f'\\nGradient Boosting Accuracy: {accuracy_score(y_test, gb_preds):.4f}')\n",
    "print(f'Gradient Boosting ROC-AUC: {roc_auc_score(y_test, gb_proba):.4f}')\n",
    "\n",
    "# Ensemble prediction (weighted average)\n",
    "ensemble_proba = 0.4 * rf_proba + 0.6 * gb_proba\n",
    "ensemble_preds = (ensemble_proba > 0.5).astype(int)\n",
    "print(f'\\nEnsemble Accuracy: {accuracy_score(y_test, ensemble_preds):.4f}')\n",
    "print(f'Ensemble ROC-AUC: {roc_auc_score(y_test, ensemble_proba):.4f}')\n",
    "\n",
    "joblib.dump((rf_model, gb_model), '../models/ensemble_models.pkl')\n",
    "\n",
    "# IMPROVED TIME SERIES: SARIMA with better preprocessing\n",
    "print(\"\\n3. Training Enhanced Time Series Model...\")\n",
    "\n",
    "# Aggregate by date with proper handling\n",
    "ts_data = df.groupby('date').agg({\n",
    "    'cumulative_rolls': 'mean',\n",
    "    'passed': 'mean',  # Success rate\n",
    "    'total_votes': 'sum'\n",
    "}).sort_index()\n",
    "\n",
    "# Fill missing dates\n",
    "date_range = pd.date_range(ts_data.index.min(), ts_data.index.max(), freq='D')\n",
    "ts_data = ts_data.reindex(date_range, method='ffill')\n",
    "\n",
    "# Check stationarity\n",
    "adf_test = adfuller(ts_data['cumulative_rolls'].dropna())\n",
    "print(f'\\nADF Statistic: {adf_test[0]:.4f}, p-value: {adf_test[1]:.4f}')\n",
    "\n",
    "# Fit SARIMA model (with seasonal component)\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    model_sarima = SARIMAX(\n",
    "        ts_data['cumulative_rolls'], \n",
    "        order=(2, 1, 1),  # ARIMA parameters\n",
    "        seasonal_order=(1, 0, 1, 7),  # Weekly seasonality\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    model_fit = model_sarima.fit(disp=False)\n",
    "    forecast = model_fit.forecast(steps=7)\n",
    "    print(f'SARIMA 7-day Forecast: {forecast.values}')\n",
    "    joblib.dump(model_fit, '../models/sarima_model.pkl')\n",
    "except:\n",
    "    print(\"SARIMA failed, using standard ARIMA\")\n",
    "    model_arima = ARIMA(ts_data['cumulative_rolls'], order=(2,1,1))\n",
    "    model_fit = model_arima.fit()\n",
    "    forecast = model_fit.forecast(steps=7)\n",
    "    print(f'ARIMA 7-day Forecast: {forecast.values}')\n",
    "    joblib.dump(model_fit, '../models/arima_model.pkl')\n",
    "\n",
    "# IMPROVED GNN: Better architecture and training\n",
    "print(\"\\n4. Training Enhanced Graph Neural Network...\")\n",
    "\n",
    "class ImprovedGCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.5):\n",
    "        super(ImprovedGCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        # Add self-loops\n",
    "        A_hat = A + torch.eye(A.size(0))\n",
    "        # Degree normalization\n",
    "        D = torch.diag(torch.sum(A_hat, dim=1).pow(-0.5))\n",
    "        A_norm = torch.matmul(torch.matmul(D, A_hat), D)\n",
    "        \n",
    "        # Graph convolution\n",
    "        out = torch.matmul(A_norm, X)\n",
    "        out = self.linear(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class ImprovedGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.5):\n",
    "        super(ImprovedGNN, self).__init__()\n",
    "        self.gcn1 = ImprovedGCNLayer(input_dim, hidden_dims[0], dropout)\n",
    "        self.gcn2 = ImprovedGCNLayer(hidden_dims[0], hidden_dims[1], dropout)\n",
    "        self.gcn3 = ImprovedGCNLayer(hidden_dims[1], hidden_dims[2], dropout)\n",
    "        \n",
    "        # Graph-level prediction layers\n",
    "        # Now operating on node features directly\n",
    "        self.fc1 = nn.Linear(hidden_dims[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        # Graph convolutions\n",
    "        x = self.gcn1(X, A)\n",
    "        x = self.gcn2(x, A)\n",
    "        x = self.gcn3(x, A)\n",
    "        \n",
    "        # Node-level predictions (no pooling)\n",
    "        # Apply FC layers to each node\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Create a more realistic graph structure\n",
    "# In practice, this should be based on actual sponsor/cosponsor relationships\n",
    "num_nodes = min(100, len(df))\n",
    "G = nx.watts_strogatz_graph(num_nodes, 20, 0.3)  # Small-world network\n",
    "adj = nx.adjacency_matrix(G).todense()\n",
    "A = torch.tensor(adj, dtype=torch.float)\n",
    "\n",
    "# Prepare node features (use selected features)\n",
    "X_gnn = torch.tensor(X_selected[:num_nodes], dtype=torch.float)\n",
    "y_gnn = torch.tensor(y.values[:num_nodes], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * num_nodes)\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[:train_size] = True\n",
    "val_mask = ~train_mask\n",
    "\n",
    "# Initialize model with better architecture\n",
    "model_gnn = ImprovedGNN(\n",
    "    input_dim=X_gnn.shape[1],\n",
    "    hidden_dims=[64, 32, 16],\n",
    "    output_dim=1,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Training with better optimization\n",
    "optimizer = torch.optim.Adam(model_gnn.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(300):\n",
    "    # Training\n",
    "    model_gnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model_gnn(X_gnn, A)\n",
    "    \n",
    "    # Only compute loss on training nodes\n",
    "    train_loss = criterion(pred[train_mask], y_gnn[train_mask])\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model_gnn.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model_gnn(X_gnn, A)\n",
    "        val_loss = criterion(val_pred[val_mask], y_gnn[val_mask])\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model_gnn.state_dict(), '../models/best_gnn_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > 20:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        train_acc = ((pred[train_mask] > 0.5) == y_gnn[train_mask]).float().mean()\n",
    "        val_acc = ((val_pred[val_mask] > 0.5) == y_gnn[val_mask]).float().mean()\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model_gnn.load_state_dict(torch.load('../models/best_gnn_model.pth'))\n",
    "print(\"\\nGNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3610600-eb20-4f73-aea9-c82df71867c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c0cf5-a595-48d4-894f-f2579bf43332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
